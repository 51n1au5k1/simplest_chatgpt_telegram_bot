# Configuration file for Simplest ChatGPT bot
# Fill in the values below to customize the bot's behavior

# The maximum number of tokens to generate in the model's response
MAX_TOKENS=2048

# The number of responses to generate from the model for each prompt
N=1

# The string to use as a delimiter to mark the end of a response
# Use "null" to indicate that no stopping condition should be used
STOP=null

# The "creativity" of the model's responses
# A lower temperature value will generate more "safe" responses,
# while a higher temperature value will generate more creative and varied responses
TEMPERATURE=0.5

# Additional settings:

# This parameter controls the nucleus sampling technique used to generate responses. 
# It filters out tokens with a cumulative probability less than top_p. 
# This can be helpful in controlling the randomness of the generated text. A value of 0.9, for example, would lead to more focused and deterministic responses, while a value of 1.0 would consider all tokens.
TOP_P=1.0

# These parameters control how the model treats tokens that are related to the prompt's presence and frequency, respectively. 
# They can be used to fine-tune the model's behavior when generating completions. 
# For example, a higher presence_penalty discourages the model from repeating phrases from the prompt, while a higher frequency_penalty discourages the model from using common phrases.
PRESENCE_PENALTY=0.0
FREQUENCY_PENALTY=0.0
